openapi: 3.0.0
info:
  title: Azure OpenAI API
  version: 0000-00-00
  description: The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
tags:
  - name: OpenAI
paths:
  /deployments/{deploymentId}/audio/transcriptions:
    post:
      tags:
        - OpenAI
      operationId: createTranscription
      summary: Transcribes audio into the input language.
      parameters:
        - name: deploymentId
          in: path
          required: true
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelDefinitions.CreateTranscriptionResponse'
        default:
          description: An unexpected error response.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/OpenAI.ErrorResponse'
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: '#/components/schemas/ModelDefinitions.CreateTranscriptionRequest'
  /deployments/{deploymentId}/audio/translations:
    post:
      tags:
        - OpenAI
      operationId: createTranslation
      summary: Transcribes audio into the input language.
      parameters:
        - name: deploymentId
          in: path
          required: true
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelDefinitions.CreateTranslationResponse'
        default:
          description: An unexpected error response.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/OpenAI.ErrorResponse'
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: '#/components/schemas/ModelDefinitions.CreateTranslationRequest'
  /deployments/{deploymentId}/chat/completions:
    post:
      tags:
        - OpenAI
      operationId: createChatCompletion
      parameters:
        - $ref: '#/components/parameters/ModelDefinitions.CreateChatCompletionRequest.deploymentId'
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelDefinitions.CreateChatCompletionResponse'
        default:
          description: An unexpected error response.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/OpenAI.ErrorResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required:
                - model
                - messages
              properties:
                model:
                  anyOf:
                    - type: string
                    - type: string
                      enum:
                        - gpt4
                        - gpt-4-0314
                        - gpt-4-0613
                        - gpt-4-32k
                        - gpt-4-32k-0314
                        - gpt-4-32k-0613
                        - gpt-3.5-turbo
                        - gpt-3.5-turbo-16k
                        - gpt-3.5-turbo-0301
                        - gpt-3.5-turbo-0613
                        - gpt-3.5-turbo-16k-0613
                  description: |-
                    ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility)
                    table for details on which models work with the Chat API.
                  x-oaiTypeLabel: string
                messages:
                  type: array
                  items:
                    $ref: '#/components/schemas/ModelDefinitions.ChatCompletionRequestMessage'
                  description: |-
                    A list of messages comprising the conversation so far.
                    [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
                  minItems: 1
                functions:
                  type: array
                  items:
                    $ref: '#/components/schemas/ModelDefinitions.ChatCompletionFunctions'
                  description: A list of functions the model may generate JSON inputs for.
                  minItems: 1
                  maxItems: 128
                function_call:
                  anyOf:
                    - type: string
                      enum:
                        - none
                        - auto
                    - $ref: '#/components/schemas/ModelDefinitions.ChatCompletionFunctionCallOption'
                  description: |-
                    Controls how the model responds to function calls. `none` means the model does not call a
                    function, and responds to the end-user. `auto` means the model can pick between an end-user or
                    calling a function.  Specifying a particular function via `{\"name":\ \"my_function\"}` forces the
                    model to call that function. `none` is the default when no functions are present. `auto` is the
                    default if functions are present.
                temperature:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.Temperature'
                  nullable: true
                  description: |-
                    What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
                    more random, while lower values like 0.2 will make it more focused and deterministic.

                    We generally recommend altering this or `top_p` but not both.
                  default: 1
                top_p:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.TopP'
                  nullable: true
                  description: |-
                    An alternative to sampling with temperature, called nucleus sampling, where the model considers
                    the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
                    the top 10% probability mass are considered.

                    We generally recommend altering this or `temperature` but not both.
                  default: 1
                n:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.N'
                  nullable: true
                  description: |-
                    How many completions to generate for each prompt.
                    **Note:** Because this parameter generates many completions, it can quickly consume your token
                    quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
                  default: 1
                max_tokens:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.MaxTokens'
                  nullable: true
                  description: |-
                    The maximum number of [tokens](/tokenizer) to generate in the completion.

                    The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
                    [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
                    for counting tokens.
                  default: 16
                stop:
                  allOf:
                    - $ref: '#/components/schemas/ModelDefinitions.Stop'
                  description: Up to 4 sequences where the API will stop generating further tokens.
                  default: null
                presence_penalty:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.Penalty'
                  nullable: true
                  description: |-
                    Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
                    in the text so far, increasing the model's likelihood to talk about new topics.

                    [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
                frequency_penalty:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.Penalty'
                  nullable: true
                  description: |-
                    Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
                    frequency in the text so far, decreasing the model's likelihood to repeat the same line
                    verbatim.

                    [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
                logit_bias:
                  type: object
                  description: |-
                    Modify the likelihood of specified tokens appearing in the completion.
                    Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an
                    associated bias value from -100 to 100. Mathematically, the bias is added to the logits
                    generated by the model prior to sampling. The exact effect will vary per model, but values
                    between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
                    should result in a ban or exclusive selection of the relevant token.
                  additionalProperties:
                    type: integer
                    format: int64
                  nullable: true
                  x-oaiTypeLabel: map
                user:
                  allOf:
                    - $ref: '#/components/schemas/ModelDefinitions.User'
                  description: |-
                    A unique identifier representing your end-user, which can help OpenAI to monitor and detect
                    abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
                stream:
                  type: boolean
                  nullable: true
                  description: |-
                    If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
                    [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
                    as they become available, with the stream terminated by a `data: [DONE]` message.
                    [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
                  default: true
  /deployments/{deploymentId}/chat/extensions/completions:
    post:
      operationId: Completions_createCompletionOnYourOwnData
      parameters:
        - $ref: '#/components/parameters/ModelDefinitions.CreateChatCompletionRequest.deploymentId'
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelDefinitions.CreateChatCompletionResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required:
                - model
                - messages
                - dataSources
              properties:
                model:
                  anyOf:
                    - type: string
                    - type: string
                      enum:
                        - gpt4
                        - gpt-4-0314
                        - gpt-4-0613
                        - gpt-4-32k
                        - gpt-4-32k-0314
                        - gpt-4-32k-0613
                        - gpt-3.5-turbo
                        - gpt-3.5-turbo-16k
                        - gpt-3.5-turbo-0301
                        - gpt-3.5-turbo-0613
                        - gpt-3.5-turbo-16k-0613
                  description: |-
                    ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility)
                    table for details on which models work with the Chat API.
                  x-oaiTypeLabel: string
                messages:
                  type: array
                  items:
                    $ref: '#/components/schemas/ModelDefinitions.ChatCompletionRequestMessage'
                  description: |-
                    A list of messages comprising the conversation so far.
                    [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
                  minItems: 1
                functions:
                  type: array
                  items:
                    $ref: '#/components/schemas/ModelDefinitions.ChatCompletionFunctions'
                  description: A list of functions the model may generate JSON inputs for.
                  minItems: 1
                  maxItems: 128
                function_call:
                  anyOf:
                    - type: string
                      enum:
                        - none
                        - auto
                    - $ref: '#/components/schemas/ModelDefinitions.ChatCompletionFunctionCallOption'
                  description: |-
                    Controls how the model responds to function calls. `none` means the model does not call a
                    function, and responds to the end-user. `auto` means the model can pick between an end-user or
                    calling a function.  Specifying a particular function via `{\"name":\ \"my_function\"}` forces the
                    model to call that function. `none` is the default when no functions are present. `auto` is the
                    default if functions are present.
                temperature:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.Temperature'
                  nullable: true
                  description: |-
                    What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
                    more random, while lower values like 0.2 will make it more focused and deterministic.

                    We generally recommend altering this or `top_p` but not both.
                  default: 1
                top_p:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.TopP'
                  nullable: true
                  description: |-
                    An alternative to sampling with temperature, called nucleus sampling, where the model considers
                    the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
                    the top 10% probability mass are considered.

                    We generally recommend altering this or `temperature` but not both.
                  default: 1
                n:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.N'
                  nullable: true
                  description: |-
                    How many completions to generate for each prompt.
                    **Note:** Because this parameter generates many completions, it can quickly consume your token
                    quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
                  default: 1
                max_tokens:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.MaxTokens'
                  nullable: true
                  description: |-
                    The maximum number of [tokens](/tokenizer) to generate in the completion.

                    The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
                    [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
                    for counting tokens.
                  default: 16
                stop:
                  allOf:
                    - $ref: '#/components/schemas/ModelDefinitions.Stop'
                  description: Up to 4 sequences where the API will stop generating further tokens.
                  default: null
                presence_penalty:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.Penalty'
                  nullable: true
                  description: |-
                    Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
                    in the text so far, increasing the model's likelihood to talk about new topics.

                    [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
                frequency_penalty:
                  oneOf:
                    - $ref: '#/components/schemas/ModelDefinitions.Penalty'
                  nullable: true
                  description: |-
                    Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
                    frequency in the text so far, decreasing the model's likelihood to repeat the same line
                    verbatim.

                    [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
                logit_bias:
                  type: object
                  description: |-
                    Modify the likelihood of specified tokens appearing in the completion.
                    Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an
                    associated bias value from -100 to 100. Mathematically, the bias is added to the logits
                    generated by the model prior to sampling. The exact effect will vary per model, but values
                    between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
                    should result in a ban or exclusive selection of the relevant token.
                  additionalProperties:
                    type: integer
                    format: int64
                  nullable: true
                  x-oaiTypeLabel: map
                user:
                  allOf:
                    - $ref: '#/components/schemas/ModelDefinitions.User'
                  description: |-
                    A unique identifier representing your end-user, which can help OpenAI to monitor and detect
                    abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
                stream:
                  type: boolean
                  nullable: true
                  description: |-
                    If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
                    [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
                    as they become available, with the stream terminated by a `data: [DONE]` message.
                    [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
                  default: true
                dataSources:
                  type: array
                  items:
                    $ref: '#/components/schemas/DataSource'
  /deployments/{deploymentId}/completions:
    post:
      tags:
        - OpenAI
      operationId: createCompletion
      parameters:
        - $ref: '#/components/parameters/ModelDefinitions.CreateCompletionRequest.deploymentId'
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelDefinitions.CreateCompletionResponse'
        default:
          description: An unexpected error response.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/OpenAI.ErrorResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ModelDefinitions.CreateCompletionRequest'
      x-oaiMeta:
        name: Create chat completion
        group: chat
        returns: |-
          Returns a [chat completion](/docs/api-reference/chat/object) object, or a streamed sequence of
          [chat completion chunk](/docs/api-reference/chat/streaming) objects if the request is streamed.
        path: create
        examples:
          - title: No streaming
            request:
              curl: |-
                curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_model_id",
                  "messages": [
                    {
                      "role": "system",
                      "content": "You are a helpful assistant."
                    },
                    {
                      "role": "user",
                      "content": "Hello!"
                    }
                  ]
              python: |-
                import os
                import openai
                openai.api_key = os.getenv("OPENAI_API_KEY")

                completion = openai.ChatCompletion.create(
                  model="VAR_model_id",
                  messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello!"}
                  ]
                )

                print(completion.choices[0].message)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const completion = await openai.chat.completions.create({
                    messages: [{ role: "system", content: "string" }],
                    model: "VAR_model_id",
                  });

                  console.log(completion.choices[0]);
                }

                main();
            response: |-
              {
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1677652288,
                "model": "gpt-3.5-turbo-0613",
                "choices": [{
                  "index": 0,
                  "message": {
                    "role": "assistant",
                    "content": "

              Hello there, how may I assist you today?",
                  },
                  "finish_reason": "stop"
                }],
                "usage": {
                  "prompt_tokens": 9,
                  "completion_tokens": 12,
                  "total_tokens": 21
                }
              }
          - title: Streaming
            request:
              curl: |-
                curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_model_id",
                  "messages": [
                    {
                      "role": "system",
                      "content": "You are a helpful assistant."
                    },
                    {
                      "role": "user",
                      "content": "Hello!"
                    }
                  ],
                  "stream": true
                }'
              python: |-
                import os
                import openai
                openai.api_key = os.getenv("OPENAI_API_KEY")

                completion = openai.ChatCompletion.create(
                  model="VAR_model_id",
                  messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello!"}
                  ],
                  stream=True
                )

                for chunk in completion:
                  print(chunk.choices[0].delta)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const completion = await openai.chat.completions.create({
                    model: "VAR_model_id",
                    messages: [
                      {"role": "system", "content": "You are a helpful assistant."},
                      {"role": "user", "content": "Hello!"}
                    ],
                    stream: true,
                  });

                  for await (const chunk of completion) {
                    console.log(chunk.choices[0].delta.content);
                  }
                }

                main();
            response: |-
              {
                "id": "chatcmpl-123",
                "object": "chat.completion.chunk",
                "created": 1677652288,
                "model": "gpt-3.5-turbo",
                "choices": [{
                  "index": 0,
                  "delta": {
                    "content": "Hello",
                  },
                  "finish_reason": "stop"
                }]
              }
  /deployments/{deploymentId}/embeddings:
    post:
      tags:
        - OpenAI
      operationId: createEmbedding
      summary: Creates an embedding vector representing the input text.
      parameters:
        - name: deploymentId
          in: path
          required: true
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelDefinitions.CreateEmbeddingResponse'
        default:
          description: An unexpected error response.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/OpenAI.ErrorResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ModelDefinitions.CreateEmbeddingRequest'
security:
  - BearerAuth: []
components:
  parameters:
    ModelDefinitions.CreateChatCompletionRequest.deploymentId:
      name: deploymentId
      in: path
      required: true
      schema:
        type: string
    ModelDefinitions.CreateCompletionRequest.deploymentId:
      name: deploymentId
      in: path
      required: true
      schema:
        type: string
  schemas:
    DataSource:
      type: object
    ModelDefinitions.ChatCompletionFunctionCallOption:
      type: object
      required:
        - name
      properties:
        name:
          type: string
          description: The name of the function to call.
    ModelDefinitions.ChatCompletionFunctionParameters:
      type: object
      additionalProperties: {}
    ModelDefinitions.ChatCompletionFunctions:
      type: object
      required:
        - name
        - parameters
      properties:
        name:
          type: string
          description: |-
            The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
            dashes, with a maximum length of 64.
        description:
          type: string
          description: |-
            A description of what the function does, used by the model to choose when and how to call the
            function.
        parameters:
          allOf:
            - $ref: '#/components/schemas/ModelDefinitions.ChatCompletionFunctionParameters'
          description: |-
            The parameters the functions accepts, described as a JSON Schema object. See the
            [guide](/docs/guides/gpt/function-calling) for examples, and the
            [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation
            about the format.\n\nTo describe a function that accepts no parameters, provide the value
            `{\"type\": \"object\", \"properties\": {}}`.
    ModelDefinitions.ChatCompletionRequestMessage:
      type: object
      required:
        - deploymentId
        - role
        - content
      properties:
        deploymentId:
          type: string
        role:
          type: string
          enum:
            - system
            - user
            - assistant
            - function
          description: The role of the messages author. One of `system`, `user`, `assistant`, or `function`.
        content:
          type: string
          nullable: true
          description: |-
            The contents of the message. `content` is required for all messages, and may be null for
            assistant messages with function calls.
        name:
          type: string
          description: |-
            The name of the author of this message. `name` is required if role is `function`, and it
            should be the name of the function whose response is in the `content`. May contain a-z,
            A-Z, 0-9, and underscores, with a maximum length of 64 characters.
        function_call:
          type: object
          description: The name and arguments of a function that should be called, as generated by the model.
          required:
            - name
            - arguments
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: |-
                The arguments to call the function with, as generated by the model in JSON format. Note that
                the model does not always generate valid JSON, and may hallucinate parameters not defined by
                your function schema. Validate the arguments in your code before calling your function.
    ModelDefinitions.ChatCompletionResponseMessage:
      type: object
      required:
        - role
        - content
      properties:
        role:
          type: string
          enum:
            - system
            - user
            - assistant
            - function
          description: The role of the author of this message.
        content:
          type: string
          nullable: true
          description: The contents of the message.
        function_call:
          type: object
          description: The name and arguments of a function that should be called, as generated by the model.
          required:
            - name
            - arguments
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: |-
                The arguments to call the function with, as generated by the model in JSON format. Note that
                the model does not always generate valid JSON, and may hallucinate parameters not defined by
                your function schema. Validate the arguments in your code before calling your function.
        content_filter_results:
          $ref: '#/components/schemas/ModelDefinitions.ContentFilterResults'
    ModelDefinitions.CompletionUsage:
      type: object
      description: Usage statistics for the completion request.
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
      properties:
        prompt_tokens:
          type: integer
          format: int64
          description: Number of tokens in the prompt.
        completion_tokens:
          type: integer
          format: int64
          description: Number of tokens in the generated completion
        total_tokens:
          type: integer
          format: int64
          description: Total number of tokens used in the request (prompt + completion).
    ModelDefinitions.ContentFilterResult:
      type: object
      required:
        - severity
        - filtered
      properties:
        severity:
          type: string
          enum:
            - safe
            - low
            - medium
            - high
        filtered:
          type: boolean
    ModelDefinitions.ContentFilterResults:
      type: object
      required:
        - sexual
        - violence
        - hate
        - self_harm
        - error
      properties:
        sexual:
          $ref: '#/components/schemas/ModelDefinitions.ContentFilterResult'
        violence:
          $ref: '#/components/schemas/ModelDefinitions.ContentFilterResult'
        hate:
          $ref: '#/components/schemas/ModelDefinitions.ContentFilterResult'
        self_harm:
          $ref: '#/components/schemas/ModelDefinitions.ContentFilterResult'
        error:
          $ref: '#/components/schemas/ModelDefinitions.ErrorBase'
    ModelDefinitions.CreateChatCompletionResponse:
      type: object
      description: Represents a chat completion response returned by model, based on the provided input.
      required:
        - id
        - object
        - created
        - model
        - promp_filter_results
        - choices
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion.
        object:
          type: string
          description: The object type, which is always `chat.completion`.
        created:
          type: integer
          format: unixtime
          description: The Unix timestamp (in seconds) of when the chat completion was created.
        model:
          type: string
          description: The model used for the chat completion.
        promp_filter_results:
          type: array
          items:
            $ref: '#/components/schemas/ModelDefinitions.ContentFilterResult'
        choices:
          type: array
          items:
            type: object
            required:
              - index
              - message
              - finish_reason
            properties:
              index:
                type: integer
                format: int64
                description: The index of the choice in the list of choices.
              message:
                $ref: '#/components/schemas/ModelDefinitions.ChatCompletionResponseMessage'
              finish_reason:
                type: string
                enum:
                  - stop
                  - length
                  - function_call
                  - content_filter
                description: |-
                  The reason the model stopped generating tokens. This will be `stop` if the model hit a
                  natural stop point or a provided stop sequence, `length` if the maximum number of tokens
                  specified in the request was reached, `content_filter` if the content was omitted due to
                  a flag from our content filters, or `function_call` if the model called a function.
              content_filter_results:
                $ref: '#/components/schemas/ModelDefinitions.ContentFilterResult'
          description: A list of chat completion choices. Can be more than one if `n` is greater than 1.
        usage:
          $ref: '#/components/schemas/ModelDefinitions.CompletionUsage'
      x-oaiMeta:
        name: The chat completion object
        group: chat
        example: ''
    ModelDefinitions.CreateCompletionRequest:
      type: object
      required:
        - model
        - prompt
      properties:
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - babbage-002
                - davinci-002
                - text-davinci-003
                - text-davinci-002
                - text-davinci-001
                - code-davinci-002
                - text-curie-001
                - text-babbage-001
                - text-ada-001
          description: |-
            ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to
            see all of your available models, or see our [Model overview](/docs/models/overview) for
            descriptions of them.
          x-oaiTypeLabel: string
        prompt:
          allOf:
            - $ref: '#/components/schemas/ModelDefinitions.Prompt'
          description: |-
            The prompt(s) to generate completions for, encoded as a string, array of strings, array of
            tokens, or array of token arrays.

            Note that <|endoftext|> is the document separator that the model sees during training, so if a
            prompt is not specified the model will generate as if from the beginning of a new document.
          default: <|endoftext|>
        suffix:
          type: string
          nullable: true
          description: The suffix that comes after a completion of inserted text.
          default: null
        temperature:
          oneOf:
            - $ref: '#/components/schemas/ModelDefinitions.Temperature'
          nullable: true
          description: |-
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
            more random, while lower values like 0.2 will make it more focused and deterministic.

            We generally recommend altering this or `top_p` but not both.
          default: 1
        top_p:
          oneOf:
            - $ref: '#/components/schemas/ModelDefinitions.TopP'
          nullable: true
          description: |-
            An alternative to sampling with temperature, called nucleus sampling, where the model considers
            the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
            the top 10% probability mass are considered.

            We generally recommend altering this or `temperature` but not both.
          default: 1
        n:
          oneOf:
            - $ref: '#/components/schemas/ModelDefinitions.N'
          nullable: true
          description: |-
            How many completions to generate for each prompt.
            **Note:** Because this parameter generates many completions, it can quickly consume your token
            quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
          default: 1
        max_tokens:
          oneOf:
            - $ref: '#/components/schemas/ModelDefinitions.MaxTokens'
          nullable: true
          description: |-
            The maximum number of [tokens](/tokenizer) to generate in the completion.

            The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
            [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
            for counting tokens.
          default: 16
        stop:
          allOf:
            - $ref: '#/components/schemas/ModelDefinitions.Stop'
          description: Up to 4 sequences where the API will stop generating further tokens.
          default: null
        presence_penalty:
          oneOf:
            - $ref: '#/components/schemas/ModelDefinitions.Penalty'
          nullable: true
          description: |-
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
            in the text so far, increasing the model's likelihood to talk about new topics.

            [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
        frequency_penalty:
          oneOf:
            - $ref: '#/components/schemas/ModelDefinitions.Penalty'
          nullable: true
          description: |-
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
            frequency in the text so far, decreasing the model's likelihood to repeat the same line
            verbatim.

            [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
        logit_bias:
          type: object
          description: |-
            Modify the likelihood of specified tokens appearing in the completion.
            Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an
            associated bias value from -100 to 100. Mathematically, the bias is added to the logits
            generated by the model prior to sampling. The exact effect will vary per model, but values
            between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
            should result in a ban or exclusive selection of the relevant token.
          additionalProperties:
            type: integer
            format: int64
          nullable: true
          x-oaiTypeLabel: map
        user:
          allOf:
            - $ref: '#/components/schemas/ModelDefinitions.User'
          description: |-
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect
            abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
        stream:
          type: boolean
          nullable: true
          description: |-
            If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
            [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data: [DONE]` message.
            [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
          default: true
        logprobs:
          type: integer
          format: int64
          nullable: true
          description: |-
            Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens.
            For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The
            API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1`
            elements in the response.

            The maximum value for `logprobs` is 5.
          default: null
        echo:
          type: boolean
          nullable: true
          description: Echo back the prompt in addition to the completion
          default: false
        best_of:
          type: integer
          format: int64
          nullable: true
          description: |-
            Generates `best_of` completions server-side and returns the "best" (the one with the highest
            log probability per token). Results cannot be streamed.

            When used with `n`, `best_of` controls the number of candidate completions and `n` specifies
            how many to return â€“ `best_of` must be greater than `n`.

            **Note:** Because this parameter generates many completions, it can quickly consume your token
            quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
          default: 1
    ModelDefinitions.CreateCompletionResponse:
      type: object
      description: |-
        Represents a completion response from the API. Note: both the streamed and non-streamed response
        objects share the same shape (unlike the chat endpoint).
      required:
        - id
        - object
        - created
        - model
        - choices
      properties:
        id:
          type: string
          description: A unique identifier for the completion.
        object:
          type: string
          description: The object type, which is always `text_completion`.
        created:
          type: integer
          format: unixtime
          description: The Unix timestamp (in seconds) of when the completion was created.
        model:
          type: string
          description: The model used for the completion.
        choices:
          type: array
          items:
            type: object
            required:
              - index
              - text
              - logprobs
              - finish_reason
            properties:
              index:
                type: integer
                format: int64
              text:
                type: string
              logprobs:
                type: object
                required:
                  - tokens
                  - token_logprobs
                  - top_logprobs
                  - text_offset
                properties:
                  tokens:
                    type: array
                    items:
                      type: string
                  token_logprobs:
                    type: array
                    items:
                      type: number
                      format: double
                  top_logprobs:
                    type: array
                    items:
                      type: object
                      additionalProperties:
                        type: integer
                        format: int64
                  text_offset:
                    type: array
                    items:
                      type: integer
                      format: int64
                nullable: true
              finish_reason:
                type: string
                enum:
                  - stop
                  - length
                  - content_filter
                description: |-
                  The reason the model stopped generating tokens. This will be `stop` if the model hit a
                  natural stop point or a provided stop sequence, or `content_filter` if content was omitted
                  due to a flag from our content filters, `length` if the maximum number of tokens specified
                  in the request was reached, or `content_filter` if content was omitted due to a flag from our
                  content filters.
          description: The list of completion choices the model generated for the input.
        usage:
          $ref: '#/components/schemas/ModelDefinitions.CompletionUsage'
        content_filter_results:
          $ref: '#/components/schemas/ModelDefinitions.ContentFilterResults'
      x-oaiMeta:
        name: The  completion object
        legacy: true
        example: ''
    ModelDefinitions.CreateEmbeddingRequest:
      type: object
      required:
        - model
        - input
      properties:
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - text-embedding-ada-002
          description: ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.
          x-oaiTypeLabel: string
        input:
          anyOf:
            - type: string
            - type: array
              items:
                type: string
            - $ref: '#/components/schemas/ModelDefinitions.TokenArray'
            - $ref: '#/components/schemas/ModelDefinitions.TokenArrayArray'
          description: |-
            Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a
            single request, pass an array of strings or array of token arrays. Each input must not exceed
            the max input tokens for the model (8191 tokens for `text-embedding-ada-002`) and cannot be an empty string.
            [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
            for counting tokens.
        user:
          $ref: '#/components/schemas/ModelDefinitions.User'
    ModelDefinitions.CreateEmbeddingResponse:
      type: object
      required:
        - object
        - model
        - data
        - usage
      properties:
        object:
          type: string
          enum:
            - embedding
          description: The object type, which is always "embedding".
        model:
          type: string
          description: The name of the model used to generate the embedding.
        data:
          type: array
          items:
            $ref: '#/components/schemas/ModelDefinitions.Embedding'
          description: The list of embeddings generated by the model.
        usage:
          type: object
          description: The usage information for the request.
          required:
            - prompt_tokens
            - total_tokens
          properties:
            prompt_tokens:
              type: integer
              format: int64
              description: The number of tokens used by the prompt.
            total_tokens:
              type: integer
              format: int64
              description: The total number of tokens used by the request.
    ModelDefinitions.CreateTranscriptionRequest:
      type: object
      required:
        - file
        - model
      properties:
        file:
          type: string
          format: binary
          description: |-
            The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4,
            mpeg, mpga, m4a, ogg, wav, or webm.
          x-oaiTypeLabel: file
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - whisper-1
          description: ID of the model to use. Only `whisper-1` is currently available.
          x-oaiTypeLabel: string
        prompt:
          type: string
          description: |-
            An optional text to guide the model's style or continue a previous audio segment. The
            [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
        response_format:
          type: string
          enum:
            - json
            - text
            - srt
            - verbose_json
            - vtt
          description: |-
            The format of the transcript output, in one of these options: json, text, srt, verbose_json, or
            vtt.
          default: json
        temperature:
          type: number
          format: double
          description: |-
            The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more
            random, while lower values like 0.2 will make it more focused and deterministic. If set to 0,
            the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to
            automatically increase the temperature until certain thresholds are hit.
          minimum: 0
          maximum: 1
          default: 0
        language:
          type: string
          description: |-
            The language of the input audio. Supplying the input language in
            [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy
            and latency.
    ModelDefinitions.CreateTranscriptionResponse:
      type: object
      required:
        - text
      properties:
        text:
          type: string
    ModelDefinitions.CreateTranslationRequest:
      type: object
      required:
        - file
        - model
      properties:
        file:
          type: string
          format: binary
          description: |-
            The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4,
            mpeg, mpga, m4a, ogg, wav, or webm.
          x-oaiTypeLabel: file
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - whisper-1
          description: ID of the model to use. Only `whisper-1` is currently available.
          x-oaiTypeLabel: string
        prompt:
          type: string
          description: |-
            An optional text to guide the model's style or continue a previous audio segment. The
            [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
        response_format:
          type: string
          enum:
            - json
            - text
            - srt
            - verbose_json
            - vtt
          description: |-
            The format of the transcript output, in one of these options: json, text, srt, verbose_json, or
            vtt.
          default: json
        temperature:
          type: number
          format: double
          description: |-
            The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more
            random, while lower values like 0.2 will make it more focused and deterministic. If set to 0,
            the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to
            automatically increase the temperature until certain thresholds are hit.
          minimum: 0
          maximum: 1
          default: 0
    ModelDefinitions.CreateTranslationResponse:
      type: object
      required:
        - text
      properties:
        text:
          type: string
    ModelDefinitions.Embedding:
      type: object
      description: Represents an embedding vector returned by embedding endpoint.
      required:
        - index
        - object
        - embedding
      properties:
        index:
          type: integer
          format: int64
          description: The index of the embedding in the list of embeddings.
        object:
          type: string
          enum:
            - embedding
          description: The object type, which is always "embedding".
        embedding:
          type: array
          items:
            type: number
            format: double
          description: |-
            The embedding vector, which is a list of floats. The length of vector depends on the model as\
            listed in the [embedding guide](/docs/guides/embeddings).
    ModelDefinitions.ErrorBase:
      type: object
      properties:
        code:
          type: string
        message:
          type: string
    ModelDefinitions.MaxTokens:
      type: integer
      format: int64
      minimum: 0
    ModelDefinitions.N:
      type: integer
      format: int64
      minimum: 1
      maximum: 128
    ModelDefinitions.Penalty:
      type: number
      format: double
      minimum: -2
      maximum: 2
    ModelDefinitions.Prompt:
      oneOf:
        - type: string
        - type: array
          items:
            type: string
        - $ref: '#/components/schemas/ModelDefinitions.TokenArray'
        - $ref: '#/components/schemas/ModelDefinitions.TokenArrayArray'
      nullable: true
    ModelDefinitions.Stop:
      oneOf:
        - type: string
        - $ref: '#/components/schemas/ModelDefinitions.StopSequences'
      nullable: true
    ModelDefinitions.StopSequences:
      type: array
      items:
        type: string
      minItems: 1
      maxItems: 4
    ModelDefinitions.Temperature:
      type: number
      format: double
      minimum: 0
      maximum: 2
    ModelDefinitions.TokenArray:
      type: array
      items:
        type: integer
        format: int64
      minItems: 1
    ModelDefinitions.TokenArrayArray:
      type: array
      items:
        $ref: '#/components/schemas/ModelDefinitions.TokenArray'
      minItems: 1
    ModelDefinitions.TopP:
      type: number
      format: double
      minimum: 0
      maximum: 1
    ModelDefinitions.User:
      type: string
    OpenAI.Error:
      type: object
      required:
        - type
        - message
        - param
        - code
      properties:
        type:
          type: string
        message:
          type: string
        param:
          type: string
          nullable: true
        code:
          type: string
          nullable: true
    OpenAI.ErrorResponse:
      type: object
      required:
        - error
      properties:
        error:
          $ref: '#/components/schemas/OpenAI.Error'
  securitySchemes:
    BearerAuth:
      type: http
      scheme: bearer
servers:
  - url: '{base_url}/openai'
    description: OpenAI Endpoint
    variables:
      base_url:
        default: ''
